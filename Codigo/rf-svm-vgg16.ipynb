{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from keras.models import Model, Sequential\nfrom keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers import BatchNormalization\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport glob\nimport cv2\nimport os\nimport seaborn as sns\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SIZE = 128\n\ntrain_images = []\ntrain_labels = [] \nfor directory_path in glob.glob(\"../input/hotels10/Train/*\"):\n    label = directory_path.split(\"\\\\\")[-1]\n    print(label)#se imprime la clase\n    for img_path in glob.glob(os.path.join(directory_path, \"*.jpg\")):\n        #print(img_path)\n        img = cv2.imread(img_path, cv2.IMREAD_COLOR)#imagen       \n        img = cv2.resize(img, (SIZE, SIZE))#ajuste de dimensiones a 128\n        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)#en RGB\n        train_images.append(img)\n        train_labels.append(label)        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images = np.array(train_images)\ntrain_labels = np.array(train_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test\ntest_images = []\ntest_labels = [] \nfor directory_path in glob.glob(\"../input/hotels10/Test/*\"):\n    fruit_label = directory_path.split(\"\\\\\")[-1]\n    for img_path in glob.glob(os.path.join(directory_path, \"*.jpg\")):\n        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n        img = cv2.resize(img, (SIZE, SIZE))\n        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n        test_images.append(img)\n        test_labels.append(fruit_label)\n        \ntest_images = np.array(test_images)\ntest_labels = np.array(test_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Sustituir nombres de clases por numeros\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nle.fit(test_labels)\ntest_labels_encoded = le.transform(test_labels)\nle.fit(train_labels)\ntrain_labels_encoded = le.transform(train_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_labels_encoded)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dividir datos\nx_train, y_train, x_test, y_test = train_images, train_labels_encoded, test_images, test_labels_encoded","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalizar valores de pixeles\nx_train, x_test = x_train / 255.0, x_test / 255.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#One hot encode\nfrom tensorflow.keras.utils import to_categorical\n\ny_train_one_hot = to_categorical(y_train)\ny_test_one_hot = to_categorical(y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(y_test_one_hot)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configuracion de la red para la extraccion de caracteristicas\nactivation = 'relu'\n\nfeature_extractor = Sequential()\nfeature_extractor.add(Conv2D(32, 3, activation = activation, padding = 'same', input_shape = (SIZE, SIZE, 3)))\nfeature_extractor.add(BatchNormalization())\n\nfeature_extractor.add(Conv2D(64, 3, activation = activation, padding = 'same', kernel_initializer = 'he_uniform'))\nfeature_extractor.add(BatchNormalization())\nfeature_extractor.add(MaxPooling2D())\n\nfeature_extractor.add(Conv2D(128, 3, activation = activation, padding = 'same', kernel_initializer = 'he_uniform'))\nfeature_extractor.add(BatchNormalization())\n\nfeature_extractor.add(Conv2D(256, 3, activation = activation, padding = 'same', kernel_initializer = 'he_uniform'))\nfeature_extractor.add(BatchNormalization())\nfeature_extractor.add(MaxPooling2D())\n\nfeature_extractor.add(Flatten())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n#Se suma la capa densa y de clasificacion a la extraccion de caracteristicas \nx = feature_extractor.output  \nx = Dense(128, activation = activation, kernel_initializer = 'he_uniform')(x)\n#Capa de clasificacion\nprediction_layer = Dense(4, kernel_regularizer=tf.keras.regularizers.l2(0.01), activation = 'softmax')(x)#SVM\n#prediction_layer = Dense(4, activation = 'softmax')(x) #NO SVM","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Modelo con la extraccion de caracteristicas y la clasificacion\ncnn_model = Model(inputs=feature_extractor.input, outputs=prediction_layer)\ncnn_model.compile(optimizer='rmsprop',loss = 'categorical_crossentropy', metrics = ['accuracy'])\nprint(cnn_model.summary()) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Entrenamiento\nhistory = cnn_model.fit(x_train, y_train_one_hot, epochs=30, validation_data = (x_test, y_test_one_hot))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Graficas\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.plot(epochs, loss, 'y', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nplt.plot(epochs, acc, 'y', label='Training acc')\nplt.plot(epochs, val_acc, 'r', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predicciones\nprediction_NN = cnn_model.predict(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(prediction_NN)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_NN = np.argmax(prediction_NN, axis=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(prediction_NN)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_NN = le.inverse_transform(prediction_NN)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluacion\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(test_labels, prediction_NN)\nprint(cm)\nsns.heatmap(cm, annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(test_labels, prediction_NN))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ejemplo de clasificacion\nn=12  \nimg = x_test[n]\nplt.imshow(img)\ninput_img = np.expand_dims(img, axis=0) #Expand dims so the input is (num images, x, y, c)\nprediction = np.argmax(cnn_model.predict(input_img))  #argmax to convert categorical back to original\nprediction = le.inverse_transform([prediction])  #Reverse the label encoder to original name\nprint(\"The prediction for this image is: \", prediction)\nprint(\"The actual label for this image is: \", test_labels[n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Extraccion de caracteristicas con RF\nX_for_RF = feature_extractor.predict(x_train) #Extraccion de caracteristicas\n\n#RANDOM FOREST\nfrom sklearn.ensemble import RandomForestClassifier\nRF_model = RandomForestClassifier(n_estimators = 50, random_state = 42)\n\n# Entrenar el modelo\nRF_model.fit(X_for_RF, y_train) \n\n#Extraccion de caracteristicas de test\nX_test_feature = feature_extractor.predict(x_test)\n#Prediccion\nprediction_RF = RF_model.predict(X_test_feature)\n#Invertir label al original\nprediction_RF = le.inverse_transform(prediction_RF)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Matriz de confusion\ncm = confusion_matrix(test_labels, prediction_RF)\nprint(cm)\nsns.heatmap(cm, annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Clasificacion de ejemplo\nn=10  \nimg = x_test[n]\nplt.imshow(img)\ninput_img = np.expand_dims(img, axis=0) \nprediction = np.argmax(cnn_model.predict(input_img))\nprediction = le.inverse_transform([prediction])  \nprint(\"The prediction for this image is: \", prediction)\nprint(\"The actual label for this image is: \", test_labels[n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Model, Sequential\nfrom keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers import BatchNormalization\nimport os\nimport seaborn as sns\nfrom keras.applications.vgg16 import VGG16","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**VGG16**","metadata":{}},{"cell_type":"code","source":"SIZE = 256  #Dimension\n\n#Labels\ntrain_images = []\ntrain_labels = [] \n\nfor directory_path in glob.glob(\"/content/Train/*\"):\n    label = directory_path.split(\"\\\\\")[-1]\n    print(label)\n    for img_path in glob.glob(os.path.join(directory_path, \"*.jpg\")):\n        #print(img_path)\n        img = cv2.imread(img_path, cv2.IMREAD_COLOR)       \n        img = cv2.resize(img, (SIZE, SIZE))\n        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n        train_images.append(img)\n        train_labels.append(label)\n\n#lists to arrays        \ntrain_images = np.array(train_images)\ntrain_labels = np.array(train_labels)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_images = []\ntest_labels = [] \nfor directory_path in glob.glob(\"/content/Test/*\"):\n    fruit_label = directory_path.split(\"\\\\\")[-1]\n    print(fruit_label)\n    for img_path in glob.glob(os.path.join(directory_path, \"*.jpg\")):\n        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n        img = cv2.resize(img, (SIZE, SIZE))\n        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n        test_images.append(img)\n        test_labels.append(fruit_label)\n\n#Convert lists to arrays                \ntest_images = np.array(test_images)\ntest_labels = np.array(test_labels)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nle.fit(test_labels)\ntest_labels_encoded = le.transform(test_labels)\nle.fit(train_labels)\ntrain_labels_encoded = le.transform(train_labels)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalizar\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\n#One hot encode \nfrom tensorflow.keras.utils import to_categorical\ny_train_one_hot = to_categorical(y_train)\ny_test_one_hot = to_categorical(y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Modelo sin clasificacion\nVGG_model = VGG16(weights='imagenet', include_top=False, input_shape=(SIZE, SIZE, 3))\n\n#Capas no entrenables\nfor layer in VGG_model.layers:\n\tlayer.trainable = False\n    \nVGG_model.summary()  ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Extraccion de caracteristicas con vgg16 y clasificacion RF\nfeature_extractor=VGG_model.predict(x_train)\n\nfeatures = feature_extractor.reshape(feature_extractor.shape[0], -1)\n\nX_for_RF = features ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#RANDOM FOREST\nfrom sklearn.ensemble import RandomForestClassifier\nRF_model = RandomForestClassifier(n_estimators = 50, random_state = 42)\n\n# Entrenar al modelo\nRF_model.fit(X_for_RF, y_train) \n\n#Extraccion de caracteristicas de tes\nX_test_feature = VGG_model.predict(x_test)\nX_test_features = X_test_feature.reshape(X_test_feature.shape[0], -1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Prediccion\nprediction_RF = RF_model.predict(X_test_features)\n#Label original\nprediction_RF = le.inverse_transform(prediction_RF)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(test_labels, prediction_RF)\nprint(cm)\nsns.heatmap(cm, annot=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ejemplo de clasificacion con VGG16 y RF\nn=np.random.randint(0, x_test.shape[0])\nimg = x_test[n]\nplt.imshow(img)\ninput_img = np.expand_dims(img, axis=0) #Expand dims so the input is (num images, x, y, c)\ninput_img_feature=VGG_model.predict(input_img)\ninput_img_features=input_img_feature.reshape(input_img_feature.shape[0], -1)\nprediction_RF = RF_model.predict(input_img_features)[0] \nprediction_RF = le.inverse_transform([prediction_RF])  #Reverse the label encoder to original name\nprint(\"The prediction for this image is: \", prediction_RF)\nprint(\"The actual label for this image is: \", test_labels[n])","metadata":{},"execution_count":null,"outputs":[]}]}